{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'E:\\\\GPImage\\\\HandleImage\\\\Train_Enhance_Image\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     4
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# 读取图片， 并将data和label分别写入\n",
    "def read_img(path):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    fileList = os.listdir(path)\n",
    "    totalNum = len(fileList)\n",
    "    \n",
    "    for i in range(0, totalNum):\n",
    "        iPath = path + str(fileList[i])\n",
    "        #  添加 文件标签\n",
    "        # 获取文件名\n",
    "        y = fileList[i]\n",
    "        #     print(y)\n",
    "    \n",
    "        # 去除文件后缀\n",
    "        y = os.path.splitext(y)\n",
    "        #     print(y)\n",
    "        y_ = y[0]\n",
    "        #     print(y_)\n",
    "    \n",
    "        # 去除文件中的数字，获取文件标签\n",
    "        # 设置标签 1为eye 2为back 3为face 4为left 5为up         \n",
    "        y_ = \"\".join(filter(str.isalpha, y_))\n",
    "        # print(type(y_))\n",
    "        if(y_ == 'eye'):\n",
    "            y_label = 1.0\n",
    "        elif(y_ == 'back'):\n",
    "            y_label = 2.0\n",
    "        elif(y_ == 'face'):\n",
    "            y_label = 3.0\n",
    "        elif(y_ == 'left'):\n",
    "            y_label = 4.0\n",
    "        else:\n",
    "            y_label = 5.0\n",
    "        \n",
    "        # 添加标签  \n",
    "        labels.append(y_label)\n",
    "    \n",
    "        #   添加文件图片数据 \n",
    "        src = cv.imread(iPath)\n",
    "        image = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "        imgs.append(image)\n",
    "    return np.asarray(imgs, np.float32), np.asarray(labels, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     6,
     8,
     13
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\n",
    "    print(\"卷积层\")\n",
    "    print(\"input_op的类型： \" + str(type(input_op)))\n",
    "    n_in = tf.Tensor.get_shape(input_op)[-1]\n",
    "    #input_op.size()[-1]\n",
    "    \n",
    "#     n_in = input_op.get_shape()[-1].value\n",
    "    print(\"n_in的大小 ： \" + str(n_in))\n",
    "#     print(\"P的类型\" + str(type(p)))\n",
    "    \n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(scope+\"w\", shape=[kh,kw,n_in,n_out], dtype=tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "#         print(\"kernel \" + str(type(kernel)))\n",
    "        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding='SAME')\n",
    "        bias_init_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32)\n",
    "        biases = tf.Variable(bias_init_val, trainable=True, name='b')\n",
    "        z = tf.nn.bias_add(conv,biases)\n",
    "        activation = tf.nn.relu(z, name=scope)\n",
    "        p += [kernel, biases]\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_op(input_op, name, n_out, p):\n",
    "    n_in = tf.Tensor.get_shape(input_op)[-1]\n",
    "    \n",
    "    with tf.name_scope(name) as scope:\n",
    "        kernel = tf.get_variable(scope+\"w\", shape=[n_in, n_out], dtype=tf.float32,\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), name='b')\n",
    "        activation = tf.nn.relu_layer(input_op, kernel, biases, name= scope)\n",
    "        p += [kernel, biases]\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     2
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mpool_op(input_op, name, kh, kw, dh, dw):\n",
    "    print(\"池化层\")\n",
    "    return tf.nn.max_pool(input_op,\n",
    "                          ksize=[1, kh, kw, 1],\n",
    "                          strides=[1, dh, dw, 1],\n",
    "                          padding='SAME',\n",
    "                          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     7,
     12,
     37,
     56
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_op(input_op,keep_prob):\n",
    "    \n",
    "    p = []\n",
    "    print(\"p 的类型\" + str(type(p)))\n",
    "    \n",
    "    # 第一段卷积的第一个卷积层 卷积核3*3，共64个卷积核（输出通道数），步长1*1\n",
    "    # input_op：256*256*1 输出尺寸256*256*64\n",
    "    conv1_1 = conv_op(input_op, name=\"conv1_1\", kh=3, kw=3, n_out=64, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv1_1 \" + str(conv1_1))\n",
    "    # 第一段卷积的第2个卷积层 卷积核3*3，共64个卷积核（输出通道数），步长1*1\n",
    "    # input_op：256*256*64 输出尺寸256*256*64\n",
    "    conv1_2 = conv_op(conv1_1, name=\"conv1_2\", kh=3, kw=3, n_out=64, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv1_2 \" + str(conv1_2))\n",
    "    # 第一段卷积的pooling层，核2*2，步长2*2\n",
    "    # input_op：256*256*64 输出尺寸128*128*64\n",
    "    pool1 = mpool_op(conv1_2, name=\"pool1\", kh=2, kw=2, dh=2, dw=2)\n",
    "    print(\"pool1 \" + str(pool1))\n",
    "    \n",
    "    # 下面是第2段卷积，包含2个卷积层和一个pooling层\n",
    "    # 第2段卷积的第一个卷积层 卷积核3*3，共128个卷积核（输出通道数），步长1*1\n",
    "    # input_op：128*128*64 输出尺寸128*128*128\n",
    "    conv2_1 = conv_op(pool1, name=\"conv2_1\", kh=3, kw=3, n_out=128, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv2_1 \" + str(conv2_1))\n",
    "    # input_op：128*128*128 输出尺寸128*128*128\n",
    "    conv2_2 = conv_op(conv2_1, name=\"conv2_2\", kh=3, kw=3, n_out=128, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv2_2 \" + str(conv2_2))\n",
    "    # input_op：128*128*128 输出尺寸64*64*128\n",
    "    pool2 = mpool_op(conv2_2, name=\"pool2\", kh=2, kw=2, dh=2, dw=2)\n",
    "    print(\"pool2 \" + str(pool2))\n",
    "    \n",
    "    # 下面是第3段卷积，包含3个卷积层和一个pooling层\n",
    "    # 第3段卷积的第一个卷积层 卷积核3*3，共256个卷积核（输出通道数），步长1*1\n",
    "    # input_op：64*64*128 输出尺寸64*64*256\n",
    "    conv3_1 = conv_op(pool2, name=\"conv3_1\", kh=3, kw=3, n_out=256, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv3_1 \" + str(conv3_1))\n",
    "    # input_op：64*64*256 输出尺寸64*64*256\n",
    "    conv3_2 = conv_op(conv3_1, name=\"conv3_2\", kh=3, kw=3, n_out=256, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv3_2 \" + str(conv3_2))\n",
    "    # input_op：64*64*256 输出尺寸64*64*256\n",
    "    conv3_3 = conv_op(conv3_2, name=\"conv3_3\", kh=3, kw=3, n_out=256, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv3_3 \" + str(conv3_3))\n",
    "    # input_op：64*64*256 输出尺寸 32*32*256\n",
    "    pool3 = mpool_op(conv3_3, name=\"pool3\", kh=2, kw=2, dh=2, dw=2)\n",
    "    print(\"pool3 \" + str(pool3))\n",
    "    \n",
    "    \n",
    "    # 下面是第4段卷积，包含3个卷积层和一个pooling层\n",
    "    # 第3段卷积的第一个卷积层 卷积核3*3，共512个卷积核（输出通道数），步长1*1\n",
    "    # input_op：32*32*256 输出尺寸32*32*512\n",
    "    conv4_1 = conv_op(pool3, name=\"conv4_1\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv4_1 \" + str(conv4_1))\n",
    "    # input_op：32*32*512 输出尺寸 32*32*512\n",
    "    conv4_2 = conv_op(conv4_1, name=\"conv4_2\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv4_2 \" + str(conv4_2))\n",
    "    # input_op：32*32*512 输出尺寸 32*32*512\n",
    "    conv4_3 = conv_op(conv4_2, name=\"conv4_3\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv4_3 \" + str(conv4_3))\n",
    "    # input_op：32*32*512 输出尺寸 16*16*512\n",
    "    pool4 = mpool_op(conv4_3, name=\"pool4\", kh=2, kw=2, dh=2, dw=2)\n",
    "    print(\"pool4 \" + str(pool4))\n",
    "    \n",
    "    # 前面4段卷积发现，VGG16每段卷积都是把图像面积变为1/4，但是通道数翻倍\n",
    "    # 因此图像tensor的总尺寸缩小一半\n",
    "\n",
    "    # 下面是第5段卷积，包含3个卷积层和一个pooling层\n",
    "    # 第3段卷积的第一个卷积层 卷积核3*3，共512个卷积核（输出通道数），步长1*1\n",
    "    # input_op：16*16*512 输出尺寸 16*16*512\n",
    "    conv5_1 = conv_op(pool4, name=\"conv5_1\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv5_1 \" + str(conv5_1))\n",
    "    # input_op：16*16*512 输出尺寸 16*16*512\n",
    "    conv5_2 = conv_op(conv5_1, name=\"conv5_2\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    # input_op：16*16*512 输出尺寸 16*16*512\n",
    "    print(\"conv5_2 \" + str(conv5_2))\n",
    "    conv5_3 = conv_op(conv5_2, name=\"conv5_3\", kh=3, kw=3, n_out=512, dh=1,\n",
    "                      dw=1, p=p)\n",
    "    print(\"conv5_3 \" + str(conv5_3))\n",
    "    # input_op：16*16*512 输出尺寸 8*8*512\n",
    "    pool5 = mpool_op(conv5_3, name=\"pool5\", kh=2, kw=2, dh=2, dw=2)\n",
    "    print(\"pool5 \" + str(pool5))\n",
    "    \n",
    "    # 将第五段卷积网络的结果扁平化\n",
    "    # reshape将每张图片变为 8*8*512 = 32768的一维向量\n",
    "    shp = pool5.get_shape()\n",
    "    \n",
    "    print(\"shp的shape: \" + str(shp))\n",
    "    \n",
    "    flattened_shape = shp[1].value * shp[2].value * shp[3].value\n",
    "    # tf.reshape(tensor, shape, name=None) 将tensor变换为参数shape的形式。\n",
    "    resh1 = tf.reshape(pool5, [-1, flattened_shape], name=\"resh1\")\n",
    "\n",
    "    print(\"resh1的类型\" + str(type(resh1)))\n",
    "    print(resh1)\n",
    "    \n",
    "    # 第一个全连接层，是一个隐藏节点数为4096的全连接层\n",
    "    # 后面接一个dropout层，训练时保留率为0.5，预测时为1.0\n",
    "    fc6 = fc_op(resh1, name=\"fc6\", n_out=4096, p=p)\n",
    "    print(\"fc6 的shape: \" + str(fc6))\n",
    "    print(\"keep_prob 的shape： \" + str(keep_prob))\n",
    "    fc6_drop = tf.nn.dropout(fc6, keep_prob, name=\"fc6_drop\")\n",
    " \n",
    "    \n",
    "    # 第2个全连接层，是一个隐藏节点数为4096的全连接层\n",
    "    # 后面接一个dropout层，训练时保留率为0.5，预测时为1.0\n",
    "    fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=4096, p=p)\n",
    "    fc7_drop = tf.nn.dropout(fc7, keep_prob, name=\"fc7_drop\")\n",
    "\n",
    "    \n",
    "    # 最后是一个1000个输出节点的全连接层\n",
    "    # 利用softmax输出分类概率\n",
    "    # argmax输出概率最大的类别\n",
    "    fc8 = fc_op(fc7_drop, name=\"fc8\", n_out=1000, p=p)\n",
    "    softmax = tf.nn.softmax(fc8)\n",
    "    predictions = tf.argmax(softmax, 1)\n",
    "    \n",
    "#     \n",
    "    \n",
    "    return predictions, softmax, fc8, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_tensorflow_run(session, target, feed, info_string): # 与AlexNet非常相似，session参数一点点区别\n",
    "    # num_steps_burn_in预热轮数，前几轮有显存加载，可以跳过时间评测\n",
    "    # 只计算10轮迭代后的计算时间\n",
    "    num_steps_burn_in = 10\n",
    "    # 总时间\n",
    "    total_duration = 0.0\n",
    "    # 总时间的平方\n",
    "    total_duration_squared = 0.0\n",
    "    # 进行num_batches + num_steps_burn_in次迭代计算\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        # 使用time.time()记录时间\n",
    "        start_time = time.time()\n",
    "        # 每次迭代通过session.run执行\n",
    "        _ = session.run(target, feed_dict=feed) # 引入feed_dict方便后面传入keep_prob来控制Dropout层的保留比率\n",
    "        duration = time.time() - start_time\n",
    "        print(str(type(duration)) + \"   time:  \" + str(duration))\n",
    "        # 在预热之后\n",
    "        if i >= num_steps_burn_in:\n",
    "            # 每10轮迭代显示当前迭代所需时间\n",
    "            if not i % 10:\n",
    "                print (str(datetime.now()) + \": step \" +\n",
    "                       str(i - num_steps_burn_in) + \n",
    "                       \", duration = \" + str(duration))\n",
    "            total_duration += duration\n",
    "            total_duration_squared += duration * duration\n",
    "    # 计算每轮迭代的平均耗时和标准差\n",
    "    mn = total_duration / num_batches\n",
    "    vr = total_duration_squared / num_batches - mn * mn\n",
    "    sd = math.sqrt(vr)\n",
    "    print (str(datetime.now()) + \": \" +\n",
    "           str(info_string) + \"across \" + \n",
    "           str(num_batches) + \" steps, \" +  \n",
    "           str(mn) + \" +/- \" + str(sd) + \"sec / batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_benchmark():\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        image, label = read_img(path)\n",
    "        print(image.shape)\n",
    "        images_ = numpy.reshape(image, [-1, 1, 256, 256])\n",
    "        print(images_.shape)\n",
    "#         -----------------------------------------------------------\n",
    "        print(str(type(images_)))# numpy\n",
    "        print(\"***********************************************************\")    \n",
    "        in_put = Variable(torch.from_numpy(images_))\n",
    "        print(\"run_benchmark中    image  \" +str(type(in_put)))\n",
    "        print(\"***********************************************************\")\n",
    "#         ----------------------------------------------------------- \n",
    "        \n",
    "        print(\"元素的类型： \" + str(type(label[-1])))\n",
    "        \n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "#         keep_prob = tf.convert_to_tensor(label)\n",
    "#         keep_prob = Variable(torch.from_numpy(label))\n",
    "#         print(\"标签的类型 ：\" + str(type(keep_prob)))\n",
    "        # keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        predictions, softmax, fc8, p = inference_op(in_put, keep_prob)  # 构建网络结构获得参数列表\n",
    "\n",
    "        print(\"...................................................\")\n",
    "        print(\"变量所占内存大小 ： \" )\n",
    "        print(sys.getsizeof(predictions))\n",
    "        print(sys.getsizeof(softmax))\n",
    "        print(sys.getsizeof(fc8))\n",
    "        print(sys.getsizeof(p))\n",
    "        print(\"...................................................\")\n",
    "        # 通过tf.session创建新的session，并通过global_variable初始化所有参数\n",
    "        # 并通过run运行该session 只有运行run并feed数据运算才真正执行\n",
    "        init = tf.global_variables_initializer()  # 初始化全局参数\n",
    "        sess = tf.Session()  # 创建session\n",
    "        sess.run(init)\n",
    "\n",
    "        time_tensorflow_run(sess, predictions, {keep_prob: 1.0}, \"Forward\")  # 预测时节点保留率\n",
    "\n",
    "        print(\"函数已大致运行完成///////////////////////////////////\")\n",
    "        print('计算VGGNet-16最后的全连接层的输出fc8的L2 loss')\n",
    "        objective = tf.nn.l2_loss(fc8)  # 计算VGGNet-16最后的全连接层的输出fc8的L2 loss\n",
    "        print('使用tf.gradients求相对于这个loss的所有模型参数的梯度')\n",
    "        grad = tf.gradients(objective, p)  # 使用tf.gradients求相对于这个loss的所有模型参数的梯度\n",
    "        print('求解梯度的操作grad')\n",
    "        time_tensorflow_run(sess, grad, {keep_prob: 0.5}, \"Forward-backward\")  # 这里的target为求解梯度的操作grad\n",
    "        print(\"运行完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(766, 256, 256)\n",
      "(766, 1, 256, 256)\n",
      "<class 'numpy.ndarray'>\n",
      "***********************************************************\n",
      "run_benchmark中    image  <class 'torch.Tensor'>\n",
      "***********************************************************\n",
      "元素的类型： <class 'numpy.float32'>\n",
      "p 的类型<class 'list'>\n",
      "卷积层\n",
      "input_op的类型： <class 'torch.Tensor'>\n",
      "n_in的大小 ： 256\n",
      "conv1_1 Tensor(\"conv1_1:0\", shape=(766, 1, 256, 64), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 64\n",
      "conv1_2 Tensor(\"conv1_2:0\", shape=(766, 1, 256, 64), dtype=float32)\n",
      "池化层\n",
      "pool1 Tensor(\"pool1:0\", shape=(766, 1, 128, 64), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 64\n",
      "conv2_1 Tensor(\"conv2_1:0\", shape=(766, 1, 128, 128), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 128\n",
      "conv2_2 Tensor(\"conv2_2:0\", shape=(766, 1, 128, 128), dtype=float32)\n",
      "池化层\n",
      "pool2 Tensor(\"pool2:0\", shape=(766, 1, 64, 128), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 128\n",
      "conv3_1 Tensor(\"conv3_1:0\", shape=(766, 1, 64, 256), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 256\n",
      "conv3_2 Tensor(\"conv3_2:0\", shape=(766, 1, 64, 256), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 256\n",
      "conv3_3 Tensor(\"conv3_3:0\", shape=(766, 1, 64, 256), dtype=float32)\n",
      "池化层\n",
      "pool3 Tensor(\"pool3:0\", shape=(766, 1, 32, 256), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 256\n",
      "conv4_1 Tensor(\"conv4_1:0\", shape=(766, 1, 32, 512), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 512\n",
      "conv4_2 Tensor(\"conv4_2:0\", shape=(766, 1, 32, 512), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 512\n",
      "conv4_3 Tensor(\"conv4_3:0\", shape=(766, 1, 32, 512), dtype=float32)\n",
      "池化层\n",
      "pool4 Tensor(\"pool4:0\", shape=(766, 1, 16, 512), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 512\n",
      "conv5_1 Tensor(\"conv5_1:0\", shape=(766, 1, 16, 512), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 512\n",
      "conv5_2 Tensor(\"conv5_2:0\", shape=(766, 1, 16, 512), dtype=float32)\n",
      "卷积层\n",
      "input_op的类型： <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "n_in的大小 ： 512\n",
      "conv5_3 Tensor(\"conv5_3:0\", shape=(766, 1, 16, 512), dtype=float32)\n",
      "池化层\n",
      "pool5 Tensor(\"pool5:0\", shape=(766, 1, 8, 512), dtype=float32)\n",
      "shp的shape: (766, 1, 8, 512)\n",
      "resh1的类型<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"resh1:0\", shape=(766, 4096), dtype=float32)\n",
      "fc6 的shape: Tensor(\"fc6:0\", shape=(766, 4096), dtype=float32)\n",
      "keep_prob 的shape： Tensor(\"Placeholder:0\", dtype=float32)\n",
      "...................................................\n",
      "变量所占内存大小 ： \n",
      "56\n",
      "56\n",
      "56\n",
      "360\n",
      "...................................................\n",
      "<class 'float'>   time:  21.965278387069702\n",
      "<class 'float'>   time:  0.8926301002502441\n",
      "<class 'float'>   time:  0.8974595069885254\n",
      "<class 'float'>   time:  0.9275212287902832\n",
      "<class 'float'>   time:  0.931511640548706\n",
      "<class 'float'>   time:  0.9295196533203125\n",
      "<class 'float'>   time:  0.9295499324798584\n",
      "<class 'float'>   time:  0.9324705600738525\n",
      "<class 'float'>   time:  0.9305126667022705\n",
      "<class 'float'>   time:  0.9335079193115234\n",
      "<class 'float'>   time:  0.9335024356842041\n",
      "2020-03-19 17:15:55.667010: step 0, duration = 0.9335024356842041\n",
      "<class 'float'>   time:  0.9355025291442871\n",
      "<class 'float'>   time:  0.9325046539306641\n",
      "<class 'float'>   time:  0.9345052242279053\n",
      "<class 'float'>   time:  0.9325063228607178\n",
      "<class 'float'>   time:  0.9384920597076416\n",
      "<class 'float'>   time:  0.9354982376098633\n",
      "<class 'float'>   time:  0.9305121898651123\n",
      "<class 'float'>   time:  0.9345030784606934\n",
      "<class 'float'>   time:  0.9335055351257324\n",
      "<class 'float'>   time:  0.931513786315918\n",
      "2020-03-19 17:16:05.008055: step 10, duration = 0.931513786315918\n",
      "<class 'float'>   time:  0.935497522354126\n",
      "<class 'float'>   time:  0.9325089454650879\n",
      "<class 'float'>   time:  0.9305131435394287\n",
      "<class 'float'>   time:  0.9355006217956543\n",
      "<class 'float'>   time:  0.9334955215454102\n",
      "<class 'float'>   time:  0.9315135478973389\n",
      "<class 'float'>   time:  0.9384899139404297\n",
      "<class 'float'>   time:  0.9345004558563232\n",
      "<class 'float'>   time:  0.9315106868743896\n",
      "<class 'float'>   time:  0.9325087070465088\n",
      "2020-03-19 17:16:14.347099: step 20, duration = 0.9325087070465088\n",
      "<class 'float'>   time:  0.932542085647583\n",
      "<class 'float'>   time:  0.9314775466918945\n",
      "<class 'float'>   time:  0.9355003833770752\n",
      "<class 'float'>   time:  0.9355003833770752\n",
      "<class 'float'>   time:  0.9315109252929688\n",
      "<class 'float'>   time:  0.9355039596557617\n",
      "<class 'float'>   time:  0.9295153617858887\n",
      "<class 'float'>   time:  0.9325113296508789\n",
      "<class 'float'>   time:  0.9354908466339111\n",
      "<class 'float'>   time:  0.9315133094787598\n",
      "2020-03-19 17:16:23.680165: step 30, duration = 0.9315133094787598\n",
      "<class 'float'>   time:  0.9325056076049805\n",
      "<class 'float'>   time:  0.9325089454650879\n",
      "<class 'float'>   time:  0.9345383644104004\n",
      "<class 'float'>   time:  0.9335057735443115\n",
      "<class 'float'>   time:  0.9364635944366455\n",
      "<class 'float'>   time:  0.9364967346191406\n",
      "<class 'float'>   time:  0.9355137348175049\n",
      "<class 'float'>   time:  0.9384803771972656\n",
      "<class 'float'>   time:  0.9364967346191406\n",
      "<class 'float'>   time:  0.9355020523071289\n",
      "2020-03-19 17:16:33.033173: step 40, duration = 0.9355020523071289\n",
      "<class 'float'>   time:  0.9394876956939697\n",
      "<class 'float'>   time:  0.9335415363311768\n",
      "<class 'float'>   time:  0.9354662895202637\n",
      "<class 'float'>   time:  0.9355003833770752\n",
      "<class 'float'>   time:  0.9345030784606934\n",
      "<class 'float'>   time:  0.938493013381958\n",
      "<class 'float'>   time:  0.9325101375579834\n",
      "<class 'float'>   time:  0.9335048198699951\n",
      "<class 'float'>   time:  0.9434785842895508\n",
      "<class 'float'>   time:  0.9394893646240234\n",
      "2020-03-19 17:16:42.402138: step 50, duration = 0.9394893646240234\n",
      "<class 'float'>   time:  0.9384927749633789\n",
      "<class 'float'>   time:  0.9355001449584961\n",
      "<class 'float'>   time:  0.9345028400421143\n",
      "<class 'float'>   time:  0.9394903182983398\n",
      "<class 'float'>   time:  0.9345085620880127\n",
      "<class 'float'>   time:  0.934497594833374\n",
      "<class 'float'>   time:  0.94248366355896\n",
      "<class 'float'>   time:  0.9394888877868652\n",
      "<class 'float'>   time:  0.9394900798797607\n",
      "<class 'float'>   time:  0.9364988803863525\n",
      "2020-03-19 17:16:51.777091: step 60, duration = 0.9364988803863525\n",
      "<class 'float'>   time:  0.9364948272705078\n",
      "<class 'float'>   time:  0.9414854049682617\n",
      "<class 'float'>   time:  0.9375278949737549\n",
      "<class 'float'>   time:  0.9374635219573975\n",
      "<class 'float'>   time:  0.9414834976196289\n",
      "<class 'float'>   time:  0.935499906539917\n",
      "<class 'float'>   time:  0.9384920597076416\n",
      "<class 'float'>   time:  0.9335052967071533\n",
      "<class 'float'>   time:  0.9374957084655762\n",
      "<class 'float'>   time:  0.9454739093780518\n",
      "2020-03-19 17:17:01.162013: step 70, duration = 0.9454739093780518\n",
      "<class 'float'>   time:  0.9414865970611572\n",
      "<class 'float'>   time:  0.9394879341125488\n",
      "<class 'float'>   time:  0.9354925155639648\n",
      "<class 'float'>   time:  0.9374966621398926\n",
      "<class 'float'>   time:  0.9464747905731201\n",
      "<class 'float'>   time:  0.9414849281311035\n",
      "<class 'float'>   time:  0.9384794235229492\n",
      "<class 'float'>   time:  0.9394898414611816\n",
      "<class 'float'>   time:  0.9444770812988281\n",
      "<class 'float'>   time:  0.9404861927032471\n",
      "2020-03-19 17:17:10.570873: step 80, duration = 0.9404861927032471\n",
      "<class 'float'>   time:  0.936497688293457\n",
      "<class 'float'>   time:  0.9394915103912354\n",
      "<class 'float'>   time:  0.9405202865600586\n",
      "<class 'float'>   time:  0.9464371204376221\n",
      "<class 'float'>   time:  0.9404864311218262\n",
      "<class 'float'>   time:  0.940507173538208\n",
      "<class 'float'>   time:  0.944488525390625\n",
      "<class 'float'>   time:  0.9454472064971924\n",
      "<class 'float'>   time:  0.9414811134338379\n",
      "<class 'float'>   time:  0.9335072040557861\n",
      "2020-03-19 17:17:19.979738: step 90, duration = 0.9335072040557861\n",
      "<class 'float'>   time:  0.9434773921966553\n",
      "<class 'float'>   time:  0.9434795379638672\n",
      "<class 'float'>   time:  0.9394969940185547\n",
      "<class 'float'>   time:  0.9444687366485596\n",
      "<class 'float'>   time:  0.9434792995452881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>   time:  0.9434800148010254\n",
      "<class 'float'>   time:  0.9374938011169434\n",
      "<class 'float'>   time:  0.9374949932098389\n",
      "<class 'float'>   time:  0.9454700946807861\n",
      "2020-03-19 17:17:28.461071: Forwardacross 100 steps, 0.9370957493782044 +/- 0.004132867398004191sec / batch\n",
      "函数已大致运行完成///////////////////////////////////\n",
      "计算VGGNet-16最后的全连接层的输出fc8的L2 loss\n",
      "使用tf.gradients求相对于这个loss的所有模型参数的梯度\n",
      "求解梯度的操作grad\n",
      "<class 'float'>   time:  30.60780692100525\n",
      "<class 'float'>   time:  2.7484309673309326\n",
      "<class 'float'>   time:  2.7656116485595703\n",
      "<class 'float'>   time:  2.7676029205322266\n",
      "<class 'float'>   time:  2.752645492553711\n",
      "<class 'float'>   time:  2.759626626968384\n",
      "<class 'float'>   time:  2.7536447048187256\n",
      "<class 'float'>   time:  2.7706289291381836\n",
      "<class 'float'>   time:  2.768571376800537\n",
      "<class 'float'>   time:  2.7716047763824463\n",
      "<class 'float'>   time:  2.758629560470581\n",
      "2020-03-19 17:18:26.853422: step 0, duration = 2.758629560470581\n",
      "<class 'float'>   time:  2.7766151428222656\n",
      "<class 'float'>   time:  2.762620687484741\n",
      "<class 'float'>   time:  2.76061749458313\n",
      "<class 'float'>   time:  2.777582883834839\n",
      "<class 'float'>   time:  2.776576280593872\n",
      "<class 'float'>   time:  2.766608238220215\n",
      "<class 'float'>   time:  2.773582696914673\n",
      "<class 'float'>   time:  2.780571460723877\n",
      "<class 'float'>   time:  2.7795872688293457\n",
      "<class 'float'>   time:  2.7745728492736816\n",
      "2020-03-19 17:18:54.586319: step 10, duration = 2.7745728492736816\n",
      "<class 'float'>   time:  2.7725918292999268\n",
      "<class 'float'>   time:  2.771595001220703\n",
      "<class 'float'>   time:  2.785557508468628\n",
      "<class 'float'>   time:  2.785557508468628\n",
      "<class 'float'>   time:  2.7676022052764893\n",
      "<class 'float'>   time:  2.7666070461273193\n",
      "<class 'float'>   time:  2.7805707454681396\n",
      "<class 'float'>   time:  2.782564640045166\n",
      "<class 'float'>   time:  2.768603801727295\n",
      "<class 'float'>   time:  2.7785768508911133\n",
      "2020-03-19 17:19:22.349139: step 20, duration = 2.7785768508911133\n",
      "<class 'float'>   time:  2.7775769233703613\n",
      "<class 'float'>   time:  2.7815706729888916\n",
      "<class 'float'>   time:  2.786552667617798\n",
      "<class 'float'>   time:  2.773589611053467\n",
      "<class 'float'>   time:  2.787550926208496\n",
      "<class 'float'>   time:  2.7725934982299805\n",
      "<class 'float'>   time:  2.7835633754730225\n",
      "<class 'float'>   time:  2.791538953781128\n",
      "<class 'float'>   time:  2.790543794631958\n",
      "<class 'float'>   time:  2.768602132797241\n",
      "2020-03-19 17:19:50.163819: step 30, duration = 2.768602132797241\n",
      "<class 'float'>   time:  2.791557550430298\n",
      "<class 'float'>   time:  2.781552314758301\n",
      "<class 'float'>   time:  2.7885475158691406\n",
      "<class 'float'>   time:  2.7825653553009033\n",
      "<class 'float'>   time:  2.7885489463806152\n",
      "<class 'float'>   time:  2.7755837440490723\n",
      "<class 'float'>   time:  2.7855584621429443\n",
      "<class 'float'>   time:  2.7775769233703613\n",
      "<class 'float'>   time:  2.788550615310669\n",
      "<class 'float'>   time:  2.78855037689209\n",
      "2020-03-19 17:20:18.012411: step 40, duration = 2.78855037689209\n",
      "<class 'float'>   time:  2.7905421257019043\n",
      "<class 'float'>   time:  2.7925586700439453\n",
      "<class 'float'>   time:  2.770582437515259\n",
      "<class 'float'>   time:  2.7855513095855713\n",
      "<class 'float'>   time:  2.7895476818084717\n",
      "<class 'float'>   time:  2.7895452976226807\n",
      "<class 'float'>   time:  2.7895476818084717\n",
      "<class 'float'>   time:  2.790544271469116\n",
      "<class 'float'>   time:  2.787552833557129\n",
      "<class 'float'>   time:  2.7955284118652344\n",
      "2020-03-19 17:20:45.893912: step 50, duration = 2.7955284118652344\n",
      "<class 'float'>   time:  2.790544033050537\n",
      "<class 'float'>   time:  2.777578115463257\n",
      "<class 'float'>   time:  2.785792350769043\n",
      "<class 'float'>   time:  2.785538911819458\n",
      "<class 'float'>   time:  2.7964279651641846\n",
      "<class 'float'>   time:  2.79952073097229\n",
      "<class 'float'>   time:  2.781566619873047\n",
      "<class 'float'>   time:  2.7885515689849854\n",
      "<class 'float'>   time:  2.797522783279419\n",
      "<class 'float'>   time:  2.7965290546417236\n",
      "2020-03-19 17:21:13.796495: step 60, duration = 2.7965290546417236\n",
      "<class 'float'>   time:  2.799518585205078\n",
      "<class 'float'>   time:  2.7925498485565186\n",
      "<class 'float'>   time:  2.7845489978790283\n",
      "<class 'float'>   time:  2.785557985305786\n",
      "<class 'float'>   time:  2.789546012878418\n",
      "<class 'float'>   time:  2.7945337295532227\n",
      "<class 'float'>   time:  2.795530319213867\n",
      "<class 'float'>   time:  2.7865536212921143\n",
      "<class 'float'>   time:  2.7945351600646973\n",
      "<class 'float'>   time:  2.790541887283325\n",
      "2020-03-19 17:21:41.709911: step 70, duration = 2.790541887283325\n",
      "<class 'float'>   time:  2.795531988143921\n",
      "<class 'float'>   time:  2.7925519943237305\n",
      "<class 'float'>   time:  2.7835488319396973\n",
      "<class 'float'>   time:  2.7875518798828125\n",
      "<class 'float'>   time:  2.786569356918335\n",
      "<class 'float'>   time:  2.7915310859680176\n",
      "<class 'float'>   time:  2.794529676437378\n",
      "<class 'float'>   time:  2.793534278869629\n",
      "<class 'float'>   time:  2.7955291271209717\n",
      "<class 'float'>   time:  2.8005175590515137\n",
      "2020-03-19 17:22:09.632305: step 80, duration = 2.8005175590515137\n",
      "<class 'float'>   time:  2.7995221614837646\n",
      "<class 'float'>   time:  2.7915430068969727\n",
      "<class 'float'>   time:  2.7855567932128906\n",
      "<class 'float'>   time:  2.7885496616363525\n",
      "<class 'float'>   time:  2.798522710800171\n",
      "<class 'float'>   time:  2.7995195388793945\n",
      "<class 'float'>   time:  2.7995193004608154\n",
      "<class 'float'>   time:  2.7995200157165527\n",
      "<class 'float'>   time:  2.8015143871307373\n",
      "<class 'float'>   time:  2.795530319213867\n",
      "2020-03-19 17:22:37.593594: step 90, duration = 2.795530319213867\n",
      "<class 'float'>   time:  2.798522472381592\n",
      "<class 'float'>   time:  2.8055057525634766\n",
      "<class 'float'>   time:  2.7945339679718018\n",
      "<class 'float'>   time:  2.800504207611084\n",
      "<class 'float'>   time:  2.7975215911865234\n",
      "<class 'float'>   time:  2.7995212078094482\n",
      "<class 'float'>   time:  2.8005151748657227\n",
      "<class 'float'>   time:  2.8045074939727783\n",
      "<class 'float'>   time:  2.8025219440460205\n",
      "2020-03-19 17:23:02.798261: Forward-backwardacross 100 steps, 2.786885025501251 +/- 0.010404249982225326sec / batch\n",
      "运行完毕\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16  # VGGNet-16模型的体积较大，如果使用较大的batch_size，GPU显存会不够用\n",
    "num_batches = 100\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
