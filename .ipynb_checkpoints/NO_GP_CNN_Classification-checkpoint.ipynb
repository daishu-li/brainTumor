{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import cv2 as cv\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mp\n",
    "# print(\"导包\")\n",
    "# def load_data(filename):\n",
    "#     \"\"\"读取数据\"\"\"\n",
    "# class CifarDAta:\n",
    "#     def _init_(self, filenames, need_shuffle):\n",
    "#         all_data = []\n",
    "#         all_labels = []\n",
    "#         for filename in filenames:\n",
    "#             data, lables = load(filename)\n",
    "#             for item, label in zip(data, labels):\n",
    "#                 if label in [0, 1]:\n",
    "#                   all_data.append(item)\n",
    "#                   all_labels.append(label)\n",
    "#         self._data = np.vstack(all_data)\n",
    "#         self._labels = np.hstack(all_labels)\n",
    "#         self._num_examples = self._data.shape[0]\n",
    "#         self.need_shuffle = need_shuffle\n",
    "#         self._indicator = 0\n",
    "#         if self.need_shuffle:\n",
    "#             self._shuffle_data()\n",
    "            \n",
    "#     def _shuffle_data(data):\n",
    "#         # [0 1 2 3 4 5] -> [5 3 2 4 0 1]\n",
    "#         p = np.random.permutation(self._num_examples)\n",
    "#         self._data =self._data[p]\n",
    "#         self._labels = self._labels[p]\n",
    "        \n",
    "#     def next_batch(self, batch_size):\n",
    "#         # 返回batch_size个样本\n",
    "#         end_indicator = self._indicator + batch_size\n",
    "#         if end_indicator > self._num_examples:\n",
    "#             if self._need_shuffle:\n",
    "#                 self._shuffle_data()\n",
    "#                 self._indicator = 0\n",
    "#                 end_indicator = batch_size\n",
    "#             else:\n",
    "#                 raise Exception(\"have no maore examples\")\n",
    "#         if end_indicator > self.num_examples:\n",
    "#             raise Exception(\"batch size is larger than all examples\")\n",
    "#         batch_data = self.data[self._indicator:end_indicator]\n",
    "#         batch_labels = self._labels(self._indicator:end_indicator)\n",
    "#         self._indicator = end_indicator\n",
    "#         return batch_data,batch_labels\n",
    "\n",
    "# train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(i, 6)]\n",
    "# test_filenames = [os.path.join(C)]\n",
    "# #  None 表示输入的样本数量不确定\n",
    "# x = tf.placeholder(tf.float32, [None, 262144])\n",
    "# y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# # 262144 * 1\n",
    "# w = tf.get_variable('w', [x.get_shape()[-1], 1],\n",
    "#                     initializer = tf.random_normal_initializer(0, 1))\n",
    "\n",
    "# # (1, )                  \n",
    "# b = tf.get_variable('b', [1],\n",
    "#                    initializer = tf.constant_initializer(0, 0))\n",
    "\n",
    "# # [None, 262144] * [262144, 1] = [None, 1]\n",
    "# y_ = tf.matmul(x, w) + b\n",
    "\n",
    "# #  [None, 1]\n",
    "# p_y_1 = tf.nn.sigmoid(y_)\n",
    "\n",
    "# # [None, 1]\n",
    "# y_reshaped = tf.reshape(y, (-1, 1))\n",
    "# y_reshaped_float = tf.cast(y_reshaped, tf.float32)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.square(y_reshaped - p_y_1))\n",
    "\n",
    "# # bool\n",
    "# predict = p_y_1 > 0.5\n",
    "# correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshaped)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "# with tf.name_scope('train_op'):\n",
    "#     train_op = tf.train.AdadeltaOptimizer(1e-3).minimize(loss))                \n",
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run([loss, accurary , train_op], feed_dict = {x:, y:})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2 as cv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'E:\\\\GPImage\\\\HandleImage\\\\Train_Enhance_Image\\\\'\n",
    "\n",
    "# 所有图片设为512*512\n",
    "w = 512\n",
    "h = 512\n",
    "\n",
    "# 灰度图片\n",
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取图片， 并将data和label分别写入\n",
    "def read_img(path):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    fileList = os.listdir(path)\n",
    "    totalNum = len(fileList)\n",
    "    \n",
    "    for i in range(0, totalNum):\n",
    "        iPath = path + str(fileList[i])\n",
    "        #  添加 文件标签\n",
    "        # 获取文件名\n",
    "        y = fileList[i]\n",
    "        #     print(y)\n",
    "    \n",
    "        # 去除文件后缀\n",
    "        y = os.path.splitext(y)\n",
    "        #     print(y)\n",
    "        y_ = y[0]\n",
    "        #     print(y_)\n",
    "    \n",
    "        # 去除文件中的数字，获取文件标签\n",
    "        # 设置标签 1为eye 2为back 3为face 4为left 5为up         \n",
    "        y_ = \"\".join(filter(str.isalpha, y_))\n",
    "        # print(type(y_))\n",
    "        if(y_ == 'eye'):\n",
    "            y_label = 1\n",
    "        elif(y_ == 'back'):\n",
    "            y_label = 2\n",
    "        elif(y_ == 'face'):\n",
    "            y_label = 3\n",
    "        elif(y_ == 'left'):\n",
    "            y_label = 4\n",
    "        else:\n",
    "            y_label = 5\n",
    "        \n",
    "        # 添加标签  \n",
    "        labels.append(y_label)\n",
    "    \n",
    "        #   添加文件图片数据 \n",
    "        src = cv.imread(iPath)\n",
    "        image = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "        imgs.append(image)\n",
    "    \n",
    "    return np.asarray(imgs, np.float32), np.asarray(labels, np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(766, 256, 256)\n",
      "(766,)\n",
      "已获取到数据\n"
     ]
    }
   ],
   "source": [
    "# 获取图片的数据和标签\n",
    "data, label = read_img(path)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "print('已获取到数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打乱顺序，生成等差数列，打乱\n",
    "num_example = data.shape[0]\n",
    "arr = np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "data = data[arr]\n",
    "label = label[arr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成训练集和测试集，取百分之80为训练集，20为测试集\n",
    "ratio = 0.8\n",
    "s = np.int(num_example * ratio)\n",
    "x_train = data[:s]\n",
    "y_train = label[:s]\n",
    "\n",
    "x_val = data[s:]\n",
    "y_val = label[s:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全连接层\n",
      "网络层结束\n",
      "\n",
      "//////////////////////////////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////////////////////\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 256, 256) for Tensor 'x_3:0', which has shape '(?, 512, 512, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-32c111eedd19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx_train_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_a\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_train_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train_a\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mac\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1074\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1076\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1077\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 256, 256) for Tensor 'x_3:0', which has shape '(?, 512, 512, 1)'"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "#                        构建网络\n",
    "# ----------------------------------------------------\n",
    "# 占位符\n",
    "x = tf.placeholder(tf.float32, shape=[None, w, h, c], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y_')\n",
    "\n",
    "# 第一层卷积层(512 -> 251）\n",
    "conv1 = tf.layers.conv2d(\n",
    "    inputs = x,\n",
    "    filters = 32,\n",
    "    kernel_size = [11,11],\n",
    "    activation = tf.nn.relu,\n",
    "    kernel_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    )\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2)\n",
    "\n",
    "# 第二个卷积层(251 -> )\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    activation=tf.nn.relu,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    " \n",
    "# 第三个卷积层(130 -> 65)\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=pool2,\n",
    "    filters=128,\n",
    "    kernel_size=[3, 3],\n",
    "    activation=tf.nn.relu,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    " \n",
    "# 第四个卷积层(65 -> 33)\n",
    "conv4 = tf.layers.conv2d(\n",
    "    inputs=pool3,\n",
    "    filters=128,\n",
    "    kernel_size=[3, 3],\n",
    "    activation=tf.nn.relu,\n",
    "    kernel_initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    " \n",
    "re1 = tf.reshape(pool4, [-1, 6 * 6 * 128])\n",
    " \n",
    "print(\"全连接层\")\n",
    "\n",
    "# 全连接层\n",
    "dense1 = tf.layers.dense(inputs=re1,\n",
    "                         units=1024,\n",
    "                         activation=tf.nn.relu,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "dense2 = tf.layers.dense(inputs=dense1,\n",
    "                         units=512,\n",
    "                         activation=tf.nn.relu,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "logits = tf.layers.dense(inputs=dense2,\n",
    "                         units=5,  # output category\n",
    "                         activation=None,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "\n",
    "print(\"网络层结束\\n\")\n",
    "\n",
    "# ---------------------------网络结束---------------------------\n",
    "\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=logits)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(logits, 1), tf.int32), y_)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# 定义一个函数，按批次取数据\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    " \n",
    " \n",
    "# 训练和测试数据，可将n_epoch设置更大一些\n",
    "print(\"//////////////////////////////////////////////\") \n",
    "n_epoch = 10\n",
    "batch_size = 64\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"//////////////////////////////////////////////\")\n",
    "for epoch in range(n_epoch):\n",
    "    start_time = time.time()\n",
    " \n",
    "    # training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "        _, err, ac = sess.run([train_op, loss, acc], feed_dict={x: x_train_a, y_: y_train_a})\n",
    "        train_loss += err;\n",
    "        train_acc += ac;\n",
    "        n_batch += 1\n",
    "    print(\"   train loss: %f\" % (train_loss / n_batch))\n",
    "    print(\"   train acc: %f\" % (train_acc / n_batch))\n",
    " \n",
    "    # validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "        err, ac = sess.run([loss, acc], feed_dict={x: x_val_a, y_: y_val_a})\n",
    "        val_loss += err;\n",
    "        val_acc += ac;\n",
    "        n_batch += 1\n",
    "    print(\"   validation loss: %f\" % (val_loss / n_batch))\n",
    "    print(\"   validation acc: %f\" % (val_acc / n_batch))\n",
    "#     #保存模型及模型参数   \n",
    "#      if epoch % 2 == 0:\n",
    "#         saver.save(sess,model_path,global_step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
