{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 所需的预测图片的地址\n",
    "backPath = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\BACK\\\\'\n",
    "eyePath = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\EYE\\\\'\n",
    "upPath = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\UP\\\\'\n",
    "leftPath = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\LEFT\\\\'\n",
    "facePath = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\FACE\\\\'\n",
    "\n",
    "# 保存训练所得model的地址\n",
    "back_model_path = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\model\\\\CNN\\\\back\\\\iris_model.ckpt'\n",
    "eye_model_path = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\model\\\\CNN\\\\eye\\\\iris_model.ckpt'\n",
    "up_model_path = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\model\\\\CNN\\\\up\\\\iris_model.ckpt'\n",
    "left_model_path = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\model\\\\CNN\\\\left\\\\iris_model.ckpt'\n",
    "face_model_path = 'E:\\\\GPImage\\\\HandleImage\\\\HandleEnhanceImage\\\\model\\\\CNN\\\\face\\\\iris_model.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  加载数据   \n",
    "class Data:\n",
    "    def __init__(self, filenames, need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        path = filenames\n",
    "        fileList = os.listdir(filenames)\n",
    "        totalNum = len(fileList)\n",
    "        for i in range(totalNum):\n",
    "            \n",
    "            iPath = path + str(fileList[i])\n",
    "            # print(iPath)\n",
    "            #  添加 文件标签\n",
    "            # 获取文件名\n",
    "            y = fileList[i]\n",
    "            # 去除文件后缀\n",
    "            y = os.path.splitext(y)\n",
    "            y_ = y[0]\n",
    "            # 去除文件中的数字，获取文件标签\n",
    "            y_ = \"\".join(filter(str.isalpha, y_))\n",
    "            if(y_ == 'meningioma'):\n",
    "                y_label = 1\n",
    "            elif(y_ == 'glioma'):\n",
    "                y_label = 2\n",
    "            else:\n",
    "                y_label = 3\n",
    "            #   添加文件图片数据 \n",
    "            src = cv.imread(iPath)\n",
    "            image = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "           \n",
    "            image = cv.resize(\n",
    "                              image, \n",
    "                              (256, 256),\n",
    "                              interpolation=cv.INTER_AREA\n",
    "                             )  \n",
    "            \n",
    "            image = image.flatten()\n",
    "            all_data.append(image)\n",
    "            all_labels.append(y_label)\n",
    "            \n",
    "            \n",
    "        self._data = np.vstack(all_data)\n",
    "        # 将数据缩放到[-1, 1]\n",
    "        self._data = self._data / 127.5 - 1\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        \n",
    "        print(self._data.shape)\n",
    "        print(self._labels.shape)\n",
    "        \n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        \n",
    "        self._indicator = 0  # 当前遍历到的位置\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "        \n",
    "        \n",
    "    \"\"\"打乱训练数据集\"\"\"\n",
    "    def _shuffle_data(self):\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    \"\"\"返回batch_size个样本\"\"\"\n",
    "    def next_batch (self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch_size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 65536], name = 'x_input')\n",
    "y = tf.placeholder(tf.int64, [None], name = 'y_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-579e327c3281>:157: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "# 各层参数设置\n",
    "x_image = tf.reshape(x, [-1, 1, 256, 256])\n",
    "# 256 * 256\n",
    "x_image = tf.transpose(x_image,  perm=[0, 2, 3, 1])\n",
    "\n",
    "conv1_1 = tf.layers.conv2d(x_image, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           name='conv1_1',\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu,\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv1_2 = tf.layers.conv2d(conv1_1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv1_2',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "# 128 * 128\n",
    "pooling1 = tf.layers.max_pooling2d(conv1_2, \n",
    "                                  (2, 2), # 核大小\n",
    "                                  (2, 2), # 步长\n",
    "                                  name='pool1')\n",
    "\n",
    "conv2_1 = tf.layers.conv2d(pooling1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv2_1',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv2_2 = tf.layers.conv2d(conv2_1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv2_2',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "# 64 * 64\n",
    "pooling2 = tf.layers.max_pooling2d(conv2_2, \n",
    "                                  (2, 2), # 核大小\n",
    "                                  (2, 2), # 步长\n",
    "                                  name='pool2')\n",
    "\n",
    "conv3_1 = tf.layers.conv2d(pooling2, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv3_1',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv3_2 = tf.layers.conv2d(conv3_1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv3_2',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv3_3 = tf.layers.conv2d(conv3_2, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv3_3',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "# 32 * 32\n",
    "pooling3 = tf.layers.max_pooling2d(conv3_3, \n",
    "                                  (2, 2), # 核大小\n",
    "                                  (2, 2), # 步长\n",
    "                                  name='pool3')\n",
    "\n",
    "# -------------------------------------------\n",
    "conv4_1 = tf.layers.conv2d(pooling3, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv4_1',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv4_2 = tf.layers.conv2d(conv4_1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv4_2',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv4_3 = tf.layers.conv2d(conv4_2, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv4_3',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "# 16 * 16\n",
    "pooling4 = tf.layers.max_pooling2d(conv4_3, \n",
    "                                  (2, 2), # 核大小\n",
    "                                  (2, 2), # 步长\n",
    "                                  name='pool4')\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "conv5_1 = tf.layers.conv2d(pooling4, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv5_1',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv5_2 = tf.layers.conv2d(conv5_1, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv5_2',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "conv5_3 = tf.layers.conv2d(conv5_2, \n",
    "                           32, # 卷积核输出的通道数\n",
    "                           (3, 3), #卷积核的大小\n",
    "                           padding='same', #使用padding 使得图片输出大小相等\n",
    "                           activation=tf.nn.relu, \n",
    "                           name='conv5_3',\n",
    "                           reuse=tf.AUTO_REUSE)\n",
    "\n",
    "# 16 * 16\n",
    "pooling5 = tf.layers.max_pooling2d(conv5_3, \n",
    "                                  (2, 2), # 核大小\n",
    "                                  (2, 2), # 步长\n",
    "                                  name='pool5')\n",
    "\n",
    "\n",
    "#[None, 32 * 32 * 32]\n",
    "flatten = tf.layers.flatten(pooling5)\n",
    "\n",
    "y_ = tf.layers.dense(flatten, 5)\n",
    "\n",
    "\n",
    "\"\"\"交叉熵损失函数\"\"\"\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "# y_ -> sofmax\n",
    "# y -> one_hot\n",
    "# loss = ylogy_\n",
    "\n",
    "\n",
    "#indices\n",
    "predict = tf.arg_max(y_, 1, name = 'output')\n",
    "corrent_preicttion = tf.equal(predict, y)\n",
    "#准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(corrent_preicttion, tf.float64))\n",
    "\n",
    "\n",
    "#梯度下降\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001 ,epsilon=1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(path, savePath):\n",
    "    # 加载数据\n",
    "    data = Data(filenames=path, need_shuffle=True)\n",
    "    test_data = Data(filenames=path, need_shuffle=False)\n",
    "\n",
    "    # 初始化函数\n",
    "    init = tf.global_variables_initializer() \n",
    "    batch_size = 50\n",
    "    batch_steps = 1000\n",
    "    test_steps = 50\n",
    "\n",
    "    # 保存精度最高的模型(最近的1次)\n",
    "#     saver = tf.train.Saver(max_to_keep=1)\n",
    "    saver_max_acc = 0\n",
    "    saver_min_loss = 1\n",
    "\n",
    "    #实例化session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for j in range(5):\n",
    "            for i in range(batch_steps):\n",
    "                batch_data, batch_labels = data.next_batch(batch_size)\n",
    "                loss_val, acc_val, _ = sess.run([loss, accuracy, train_op], feed_dict={x: batch_data, y: batch_labels})\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print('[Train] Step: %d 批  %d, loss: %4.5f, acc: %4.5f' % (j+1, i+1, loss_val, acc_val))\n",
    "\n",
    "                #每500次进行一次测试\n",
    "                if (i+1) % 500 == 0:\n",
    "\n",
    "                    all_test_acc_val = []\n",
    "                    all_test_loss = []\n",
    "                    for j in range(test_steps):\n",
    "                        test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                        test_acc_val, test_loss_val = sess.run([accuracy, loss], feed_dict = {x: test_batch_data, y: test_batch_labels})\n",
    "                        all_test_acc_val.append(test_acc_val)\n",
    "                        all_test_loss.append(test_loss_val)\n",
    "\n",
    "                    test_acc = np.mean(all_test_acc_val)\n",
    "                    test_loss = np.mean(all_test_loss)\n",
    "                    print(\"----------------------------------------\")\n",
    "                    print('[Test] Step: %d 批  %d, loss: %4.5f, acc: %4.5f' %( j+1, i+1, test_loss, test_acc))\n",
    "                    print(\"----------------------------------------\")\n",
    "\n",
    "                    if(test_acc > saver_max_acc and test_loss < saver_min_loss):\n",
    "                        saver = tf.train.Saver(max_to_keep=1)\n",
    "                        saver.save(sess, savePath, global_step=i+1)\n",
    "                        print('模型已保存')\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5522, 65536)\n",
      "(5522,)\n",
      "(5522, 65536)\n",
      "(5522,)\n",
      "[Train] Step: 1 批  100, loss: 0.14552, acc: 0.96000\n",
      "[Train] Step: 1 批  200, loss: 0.13217, acc: 0.92000\n",
      "[Train] Step: 1 批  300, loss: 0.07638, acc: 0.98000\n",
      "[Train] Step: 1 批  400, loss: 0.06424, acc: 0.98000\n",
      "[Train] Step: 1 批  500, loss: 0.02904, acc: 1.00000\n",
      "----------------------------------------\n",
      "[Test] Step: 50 批  500, loss: 0.02002, acc: 0.99480\n",
      "----------------------------------------\n",
      "模型已保存\n",
      "[Train] Step: 50 批  600, loss: 0.01504, acc: 0.98000\n",
      "[Train] Step: 50 批  700, loss: 0.06096, acc: 0.96000\n",
      "[Train] Step: 50 批  800, loss: 0.05010, acc: 0.98000\n",
      "[Train] Step: 50 批  900, loss: 0.00337, acc: 1.00000\n",
      "[Train] Step: 50 批  1000, loss: 0.00150, acc: 1.00000\n",
      "----------------------------------------\n",
      "[Test] Step: 50 批  1000, loss: 0.00357, acc: 0.99840\n",
      "----------------------------------------\n",
      "模型已保存\n",
      "[Train] Step: 2 批  100, loss: 0.01592, acc: 0.98000\n",
      "[Train] Step: 2 批  200, loss: 0.00244, acc: 1.00000\n",
      "[Train] Step: 2 批  300, loss: 0.03568, acc: 1.00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e4d6d5160928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# get_model(facePath, face_model_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# get_model(backPath, back_model_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleftPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6706587812f8>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(path, savePath)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[Train] Step: %d 批  %d, loss: %4.5f, acc: %4.5f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda\\envs\\GP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get_model(eyePath, eye_model_path)\n",
    "# get_model(upPath, up_model_path)\n",
    "# get_model(facePath, face_model_path)\n",
    "# get_model(backPath, back_model_path)\n",
    "# get_model(leftPath, left_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
